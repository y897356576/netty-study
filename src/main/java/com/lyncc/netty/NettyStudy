服务端：
EchoServerHandler 实现了业务逻辑；
main()方法引导了服务器；
引导过程中所需要的步骤如下：
创建一个ServerBootstrap 的实例以引导和绑定服务器；
创建并分配一个NioEventLoopGroup 实例以进行事件的处理，如接受新连接以及读/写数据；
指定服务器绑定的本地的InetSocketAddress；
使用一个EchoServerHandler 的实例初始化每一个新的Channel；
调用ServerBootstrap.bind()方法以绑定服务器。

客户端：
为初始化客户端，创建了一个Bootstrap 实例；
为进行事件处理分配了一个NioEventLoopGroup 实例，其中事件处理包括创建新的连接以及处理入站和出站数据；
为服务器连接创建了一个InetSocketAddress 实例；
当连接被建立时，一个EchoClientHandler 实例会被安装到（该Channel 的）ChannelPipeline 中；
调用Bootstrap.connect()方法连接到远程节点。


Channel—Socket
EventLoop—控制流、多线程处理、并发
ChannelFuture—异步通知

Channel:（对socket的封装）
基本的I/O 操作（bind()、connect()、read()和write()）依赖于底层网络传输所提供的原语。
在基于Java 的网络编程中，其基本的构造是class Socket。
Netty 的Channel 接口所提供的API，大大地降低了直接使用Socket 类的复杂性。
[新建的Channel将会被绑定在一个selector上]

EventLoop:
[每一个EventLoop(Executor执行器)中都维护了两个任务队列、一个Thread、一个selector]
[两个队列包括任务队列与延迟任务队列]
[由一个Thread处理队列内的任务，消除了线程间的切换]
[队列中存放的Task是非IO操作]
[非IO操作：bind，register，channelRead，channelActive等；IO操作：OP_ACCEPT、OP_CONNECT、OP_READ、OP_WRITE]
[服务端的EventLoopGroup可以绑定两个，一个bossGroup，一个workerGroup]
[bossGroup负责监听新的连接、创建Channel并将Channel绑定到workerLoop并启动workerLoop的线程]
处理连接的生命周期中所发生的事件：
一个EventLoopGroup 包含一个或者多个EventLoop
一个EventLoop 在它的生命周期内只和一个Thread 绑定
所有由EventLoop 处理的I/O 事件都将在它专有的Thread 上被处理
一个EventLoop 可能会被分配一个或多个Channel
一个Channel 在它的生命周期内只注册于一个EventLoop
（在这种设计中，一个给定Channel 的I/O 操作都是由相同的Thread 执行的，实际上消除了对于同步的需要）

ChannelFuture:
Netty 中所有的I/O 操作都是异步的。（需要在之后的某个时间点确定其结果）
Netty 提供了ChannelFuture 接口，其addListener()方法注册了一个ChannelFutureListener，
以便在某个操作完成时（无论是否成功）得到通知。


ChannelHandler:
从应用程序开发人员的角度来看，Netty 的主要组件是ChannelHandler，它充当了所有处理入站和出站数据的应用程序逻辑的容器。
应用程序的业务逻辑通常驻留在一个或者多个ChannelHandler 中。

ChannelPipeline:
提供了ChannelHandler 链的容器；
当每个Channel 被创建时，它会被自动地分配到它专属的ChannelPipeline。

ChannelHandler 安装到ChannelPipeline 中的过程如下所示：
一个ChannelInitializer的实现被注册到了ServerBootstrap中；
当ChannelInitializer.initChannel()方法被调用时（接入了新的链接），会生成一个新的Channel，
    ChannelInitializer将在Channel的ChannelPipeline 中安装一组自定义的ChannelHandler；
ChannelInitializer 将它自己从ChannelPipeline 中移除（ChannelInitializer本身也是一个ChannelHandler）。


有两种类型的引导器：
一种用于服务器（ServerBootstrap）
    [绑定一个端口]
    [需要两个EventLoopGroup(可以是同一个实例)，一个处理监听Channel(负责创建新Channel)，一个处理链接Channel]
一种用于客户端（Bootstrap）
    [绑定ip + 端口]
    [需要一个EventLoopGroup处理链接Channel]



NIO选择器：
NIO 提供了一个所有I/O 操作的全异步的实现。它利用了自NIO 子系统被引入JDK 1.4 时便可用的基于选择器的API。
选择器背后的基本概念是充当一个注册表，在那里你将可以请求在Channel 的状态发生变化时得到通知。
可能的状态变化有：
新的Channel 已被接受并且就绪；
Channel 连接已经完成；
Channel 有已经就绪的可供读取的数据；
Channel 可用于写数据。
选择器运行在一个检查状态变化并对其做出相应响应的线程上，在应用程序对状态的改变做出响应之后，选择器将会被重置，并将重复这个过程。

Epoll—用于Linux的非阻塞传输
要在中使用epoll替代NIO，只需要将NioEventLoopGroup替换为EpollEventLoopGroup，
并且将NioServerSocketChannel.class 替换为EpollServerSocketChannel.class 即可。

OIO—旧的阻塞I/O

NIO与Epoll的zore-copy:
可以快速高效地将数据从文件系统移动到网络接口，而不需要将其从内核空间复制到用户空间，其在像FTP或者HTTP这样的协议中可以显著地提升性能。
但是，并不是所有的操作系统都支持这一特性。它对于实现了数据加密或者压缩的文件系统是不可用的——只能传输文件的原始内容。
反过来说，传输已被加密的文件则不是问题。


Netty的数据容器——ByteBuf
Java NIO提供的ByteBuffer类使用起来过于复杂。
ByteBuf维护了两个指针，一个读指针，一个写指针；原生态的ByteBuffer只维护了一个指针，你需要调用flip方法来切换读写的状态。



Netty的server端流程：
1）先初始化好boss和worker的NioEventLoopGroup，并初始化好Group中的每一个NioEventLoop，为每一个NioEventLoop都穿件一个selector对象
2）Netty会在bind的动作上，去让boss的NioserverSocketChannel去绑定selector，并启动与boss捆绑在一起的thread，进入无尽的循环
3）在这个生命不息，循环不止的方法中，主要做了两件事情，1是去select，不管是selectNow()方法还是select()方法，其主要目的就是去查看boss关注的selector是否有事件发生
4）当有事件发生的时候，一般就是因为有client链接，如果有链接，boss线程就需要做的事情就是初始化封装好新来的SocketChannel
5）封装好的NioSocketChannel也会有自己的Unsafe对象，这个对象是用来做一些其他的操作的，例如读操作，这与boss的Unsafe对象不一样，boss的Unsafe对象是NioMessageUnsafe是用来进行绑定channel
6）NioSocketChannel用Boss线程管道中的ServerBootstrapAcceptor对象绑定确定属于自己的worker线程之后，绑定好worker线程的selector之后就开始调用自己的run方法用来监听selector上的IO事件
7）要说明白的一点就是一个worker处理的channel在多链接的场景下，一个worker会处理不止一个SocketChannel



https://blog.csdn.net/linuu/article/list/3